{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import math\n",
    "import einops\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.ln = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(x)\n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, T: int, output_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        position = torch.arange(T).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, output_dim, 2) * (-math.log(10000.0) / output_dim))\n",
    "        pe = torch.zeros(T, output_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.pe[x].reshape(x.shape[0], self.output_dim)\n",
    "    \n",
    "# print(PositionalEmbedding(2,4)(torch.tensor([[1], [0], [0]])))\n",
    "# torch.tensor([[1,2,3], [4,5,6], [7,8,9]])[ torch.tensor([[1], [0], [1]]) ].reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, emb_dim: int, input_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0\n",
    "        head_dim = emb_dim // n_heads\n",
    "        self.K_W = nn.Parameter(torch.rand(n_heads, input_dim, head_dim))\n",
    "        self.K_b = nn.Parameter(torch.rand(n_heads, head_dim))\n",
    "        self.Q_W = nn.Parameter(torch.rand(n_heads, input_dim, head_dim))\n",
    "        self.Q_b = nn.Parameter(torch.rand(n_heads, head_dim))\n",
    "        self.V_W = nn.Parameter(torch.rand(n_heads, input_dim, head_dim))\n",
    "        self.V_b = nn.Parameter(torch.rand(n_heads, head_dim))\n",
    "        self.O_W = nn.Parameter(torch.rand(n_heads, head_dim, input_dim))\n",
    "        self.O_b = nn.Parameter(torch.rand(input_dim))\n",
    "        self.norm = nn.LayerNorm([input_dim])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm([input_dim]),\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, condition, mask):\n",
    "        _, input_dim, h, w = x.shape\n",
    "        x = einops.rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        res = x\n",
    "        res = self.norm(x)\n",
    "        k = einops.einsum(res, self.K_W, \"b size ch, n_h ch h_dim -> b size n_h h_dim\")\n",
    "        k = k + self.K_b\n",
    "        q = einops.einsum(res, self.Q_W, \"b size ch, n_h ch h_dim -> b size n_h h_dim\")\n",
    "        q = q + self.Q_b\n",
    "        qk = einops.einsum(q, k, \"batch s1 n_h h_dim, batch s2 n_h h_dim -> batch n_h s1 s2\")\n",
    "        qk = qk / torch.sqrt(torch.tensor(input_dim, dtype=torch.float))\n",
    "        qk = qk.softmax(-1)\n",
    "        v = einops.einsum(res, self.V_W, \"b size ch, n_h ch h_dim -> b size n_h h_dim\")\n",
    "        v = v + self.V_b\n",
    "        res = einops.einsum(qk, v, \"batch n_h size size, batch size n_h h_dim -> batch size n_h h_dim\")\n",
    "        res = einops.einsum(res, self.O_W, \"batch size n_h h_dim, n_h h_dim ch -> batch size ch\")\n",
    "        res = res + self.O_b\n",
    "        res = res + x\n",
    "        res = self.mlp(res) + res\n",
    "        res = einops.rearrange(res, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "        return res\n",
    "\n",
    "# (MultiheadAttention(16, 64, 128)(torch.rand((6, 128, 7, 7)), torch.rand((6, 128)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim, is_residual=False, is_debug=False):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.GroupNorm(8, in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.time_emb = MLP(input_dim=time_emb_dim, output_dim=out_channels)\n",
    "        self.condition_emb = MLP(input_dim=time_emb_dim, output_dim=out_channels)\n",
    "        if in_channels != out_channels:\n",
    "            self.conv_3 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.conv_3 = nn.Identity()\n",
    "        self.is_debug = is_debug\n",
    "        self.is_residual = is_residual\n",
    "\n",
    "    def forward(self, x, t, condition, mask):\n",
    "        h = self.conv_1(x)\n",
    "        if t is None:\n",
    "            return self.conv_3(x) + self.conv_2(h)\n",
    "        t = self.time_emb(t)\n",
    "        # print(mask[:, None].shape)\n",
    "        condition = self.condition_emb(condition) * mask[:, None]\n",
    "        batch_size, emb_dim = t.shape \n",
    "        t = t.view(batch_size, emb_dim, 1, 1)\n",
    "        condition = condition.view(batch_size, emb_dim, 1, 1)\n",
    "        conv_res = self.conv_2(h + t + condition)\n",
    "        if self.is_residual:\n",
    "            return self.conv_3(x) + conv_res\n",
    "        else:\n",
    "            return conv_res\n",
    "\n",
    "# print(ResnetBlock(8, 16, 4)(torch.rand(2, 8, 32, 32), torch.rand(2, 4), torch.rand(2,4), torch.zeros(2)).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "    \n",
    "    def forward(self, x, t, condition, mask):\n",
    "        return self.pool(x)\n",
    "    \n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upscale = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x, t, condition, mask):\n",
    "        return self.upscale(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceWithTimeEmbedding(nn.Module):\n",
    "    def __init__(self, blocks):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(blocks)\n",
    "    \n",
    "    def forward(self, x, t, cond, mask):\n",
    "        for model in self.models:\n",
    "            x = model(x, t, cond, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            T,\n",
    "            num_classes,\n",
    "            steps=(1, 2, 4),\n",
    "            hid_size = 128,\n",
    "            attn_steps = [2],\n",
    "            use_time_emb=True,\n",
    "            has_residuals=True,\n",
    "            use_self_attention=False,\n",
    "            num_resolution_blocks=2,\n",
    "            is_debug = False\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_time_emb = use_time_emb\n",
    "\n",
    "        time_emb_dim = hid_size * 4\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            PositionalEmbedding(T=T, output_dim=hid_size),\n",
    "            nn.Linear(hid_size, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.cond_embedding = nn.Sequential(\n",
    "            nn.Embedding(num_classes, hid_size),\n",
    "            nn.Linear(hid_size, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.first_conv = nn.Conv2d(in_channels, steps[0] * hid_size, 3, padding=1)\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        prev_hid_size = steps[0] * hid_size\n",
    "        for (index, step) in enumerate(steps):\n",
    "            res_blocks = []\n",
    "            for block in range(num_resolution_blocks):\n",
    "                res_blocks.append(\n",
    "                    ResnetBlock(\n",
    "                        in_channels=prev_hid_size if block == 0 else step * hid_size,\n",
    "                        out_channels=step * hid_size,\n",
    "                        time_emb_dim=time_emb_dim,\n",
    "                        is_residual=has_residuals\n",
    "                    )\n",
    "                )\n",
    "                if step in attn_steps:\n",
    "                    res_blocks.append(\n",
    "                        MultiheadAttention(\n",
    "                            n_heads=4,\n",
    "                            emb_dim=step * hid_size,\n",
    "                            input_dim=step * hid_size\n",
    "                        )\n",
    "                    )\n",
    "            self.down_blocks.append(\n",
    "                SequenceWithTimeEmbedding(res_blocks)\n",
    "            )\n",
    "            if index != len(steps) - 1:\n",
    "                self.down_blocks.append(DownBlock())\n",
    "            prev_hid_size = step * hid_size\n",
    "        if len(attn_steps) > 0:\n",
    "            self.backbone = SequenceWithTimeEmbedding([\n",
    "                ResnetBlock(steps[-1] * hid_size, steps[-1] * hid_size, time_emb_dim=time_emb_dim),\n",
    "                MultiheadAttention(n_heads=4, emb_dim=steps[-1] * hid_size, input_dim=steps[-1] * hid_size),\n",
    "                ResnetBlock(steps[-1] * hid_size, steps[-1] * hid_size, time_emb_dim=time_emb_dim),\n",
    "            ])\n",
    "        else:\n",
    "            self.backbone = SequenceWithTimeEmbedding([\n",
    "                ResnetBlock(steps[-1] * hid_size, steps[-1] * hid_size, time_emb_dim=time_emb_dim),\n",
    "                ResnetBlock(steps[-1] * hid_size, steps[-1] * hid_size, time_emb_dim=time_emb_dim),\n",
    "            ])\n",
    "\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        reverse_steps = list(reversed(steps))\n",
    "        for (index, step) in enumerate(reverse_steps):\n",
    "            res_blocks = []\n",
    "            for block in range(num_resolution_blocks):\n",
    "                next_hid_size = reverse_steps[index + 1] * hid_size if index != len(steps) - 1 else step * hid_size\n",
    "                res_blocks.append(\n",
    "                    ResnetBlock(\n",
    "                        in_channels=prev_hid_size * 2 if block == 0 else next_hid_size,\n",
    "                        out_channels=next_hid_size,\n",
    "                        time_emb_dim=time_emb_dim,\n",
    "                        is_residual=has_residuals\n",
    "                    )\n",
    "                )\n",
    "                if step in attn_steps:\n",
    "                    res_blocks.append(\n",
    "                        MultiheadAttention(\n",
    "                            n_heads=4,\n",
    "                            emb_dim=next_hid_size,\n",
    "                            input_dim=next_hid_size\n",
    "                        )\n",
    "                    )\n",
    "            self.up_blocks.append(\n",
    "                SequenceWithTimeEmbedding(res_blocks)\n",
    "            )\n",
    "            if index != len(steps) - 1:\n",
    "                self.up_blocks.append(UpBlock(next_hid_size, next_hid_size))\n",
    "            prev_hid_size = next_hid_size\n",
    "\n",
    "        self.is_debug = is_debug\n",
    "        self.out = nn.Sequential(*[\n",
    "            nn.GroupNorm(8, steps[0] * hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=steps[0] * hid_size, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t, cond, mask):\n",
    "        time_emb = self.time_embedding(t)\n",
    "        cond_emb = self.cond_embedding(cond)\n",
    "\n",
    "        x = self.first_conv(x)\n",
    "        hx = []\n",
    "        for down_block in self.down_blocks:\n",
    "            x = down_block(x, time_emb, cond_emb, mask)\n",
    "            if not isinstance(down_block, DownBlock):\n",
    "                hx.append(x)\n",
    "        x = self.backbone(x, time_emb, cond_emb, mask)\n",
    "\n",
    "        ind = len(hx) - 1\n",
    "        for up_block in self.up_blocks:\n",
    "            if not isinstance(up_block, UpBlock):\n",
    "                x = up_block(torch.cat([x, hx[ind]], 1), time_emb, cond_emb, mask)\n",
    "                ind -= 1\n",
    "            else:\n",
    "                x = up_block(x, time_emb, cond_emb, mask)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# test_unet = UNet(in_channels=1, out_channels=1, T=100, num_classes=10, is_debug=True, use_self_attention=True)\n",
    "# test_unet(torch.rand(3, 1, 28, 28), torch.randint(1, 100, (3,1)), torch.randint(1,10, (3,1)), torch.ones((3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "# Empty the PyTorch cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "class UnNormalize(torch.nn.Module):\n",
    "    def __init__(self, mean, std) -> None:\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        mean = self.mean.to(tensor.device)\n",
    "        std = self.std.to(tensor.device)\n",
    "        \n",
    "        # Clone the input tensor to avoid modifying the original tensor\n",
    "        tensor = tensor.clone()\n",
    "        \n",
    "        # Apply the un-normalization\n",
    "        unnormalized_tensor = tensor * std[:, None, None] + mean[:, None, None]\n",
    "        \n",
    "        return unnormalized_tensor\n",
    "    \n",
    "transform_to_pil = transforms.Compose([\n",
    "    # UnNormalize(mean=[0.5], std=[0.5]),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "# plt.imshow(transform_to_pil(train_dataset[0][0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root=\"./datasets\", download=True, transform=transform_to_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./datasets\", download=True, transform=transform_to_tensor, train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE MPS\n"
     ]
    }
   ],
   "source": [
    "import torch.backends\n",
    "import torch.backends.mps\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"USE MPS\")\n",
    "    # test_unet = UNet(in_channels=1, out_channels=1, is_debug=True).to(device)\n",
    "    # test_unet(train_dataset[0][0].unsqueeze(0).to(device), torch.tensor([[1]], dtype=torch.float, device=device))\n",
    "\n",
    "# import os\n",
    "# os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, T: int, nn_model: nn.Module, p_cond=0.2):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.nn_model = nn_model.to(device)\n",
    "        beta_schedule = torch.linspace(1e-4, 0.02, T + 1, device=device)\n",
    "        self.p_cond = torch.tensor([p_cond]).to(device)\n",
    "        alpha_t_schedule = 1 - beta_schedule\n",
    "        bar_alpha_t_schedule = torch.cumprod(alpha_t_schedule.detach().cpu(), 0).to(device)\n",
    "        sqrt_bar_alpha_t_schedule = torch.sqrt(bar_alpha_t_schedule)\n",
    "        sqrt_minus_bar_alpha_t_schedule = torch.sqrt(1 - bar_alpha_t_schedule)\n",
    "        self.register_buffer(\"beta_schedule\", beta_schedule)\n",
    "        self.register_buffer(\"alpha_t_schedule\", alpha_t_schedule)\n",
    "        self.register_buffer(\"bar_alpha_t_schedule\", bar_alpha_t_schedule)\n",
    "        self.register_buffer(\"sqrt_bar_alpha_t_schedule\", sqrt_bar_alpha_t_schedule)\n",
    "        self.register_buffer(\"sqrt_minus_bar_alpha_t_schedule\", sqrt_minus_bar_alpha_t_schedule)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor, conds: torch.Tensor):\n",
    "        t = torch.randint(low=1, high=self.T+1, size=(imgs.shape[0],), device=device)\n",
    "        noise = torch.randn_like(imgs, device=device)\n",
    "        batch_size, channels, width, height = imgs.shape\n",
    "        noise_imgs = self.sqrt_bar_alpha_t_schedule[t].view((batch_size, 1, 1 ,1)) * imgs \\\n",
    "            + self.sqrt_minus_bar_alpha_t_schedule[t].view((batch_size, 1, 1, 1)) * noise\n",
    "        \n",
    "        conds = conds.unsqueeze(1)\n",
    "        mask = torch.rand_like(conds, dtype=torch.float32) > self.p_cond\n",
    "        \n",
    "        pred_noise = self.nn_model(noise_imgs, t.unsqueeze(1), conds, mask.long())\n",
    "\n",
    "        return self.criterion(pred_noise, noise)\n",
    "    \n",
    "    def sample(self, n_samples: int, size, classes: List[int], w: float):\n",
    "        self.eval()\n",
    "        assert len(classes) == n_samples\n",
    "        with torch.no_grad():\n",
    "            x_t = torch.randn(n_samples, *size, device=device)\n",
    "            cond = torch.tensor(classes).unsqueeze(1).to(device)\n",
    "            mask_ones = torch.ones_like(cond)\n",
    "            mask_zeros = torch.zeros_like(cond)\n",
    "            w = torch.tensor(w).to(device)\n",
    "            for t in range(self.T, 0, -1):\n",
    "                z = torch.randn_like(x_t, device=device) if t > 0 else 0\n",
    "                t_tensor = torch.tensor([t], device=device).repeat(x_t.shape[0], 1)\n",
    "                pred_noise_cond = self.nn_model(x_t, t_tensor, cond, mask_ones)\n",
    "                pred_noise_zero = self.nn_model(x_t, t_tensor, cond, mask_zeros)\n",
    "                pred_noise = (1 + w) * pred_noise_cond - w * pred_noise_zero\n",
    "                x_t = 1 / torch.sqrt(self.alpha_t_schedule[t]) * \\\n",
    "                    (x_t - pred_noise * (1 - self.alpha_t_schedule[t]) / self.sqrt_minus_bar_alpha_t_schedule[t]) + \\\n",
    "                    torch.sqrt(self.beta_schedule[t]) * z\n",
    "            x_t = x_t*-1 + 1\n",
    "            return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: DDPM, optimizer: torch.optim.Optimizer, epochs: int):\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train(True)\n",
    "        training_loss = 0\n",
    "        val_loss = 0\n",
    "        pbar = tqdm.tqdm(train_dataloader)\n",
    "        for index, (imgs, labels) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            loss = model(imgs, labels)\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "            pbar.set_description(f\"loss for epoch {epoch}: {training_loss / (index + 1):.4f}\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (imgs, labels) in test_dataloader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                loss = model(imgs, labels)\n",
    "        \n",
    "                val_loss += loss.item()\n",
    "        training_losses.append(training_loss / len(train_dataset))\n",
    "        val_losses.append(val_loss / len(test_dataset))\n",
    "    return training_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss for epoch 0: 0.2100:  11%|█         | 51/469 [01:12<09:55,  1.42s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m      5\u001b[0m model_with_time \u001b[38;5;241m=\u001b[39m DDPM(T \u001b[38;5;241m=\u001b[39m T, nn_model\u001b[38;5;241m=\u001b[39mUNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, T\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m _, val_losses \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m      7\u001b[0m   model_with_time,\n\u001b[1;32m      8\u001b[0m   torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel_with_time\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m),\n\u001b[1;32m      9\u001b[0m   EPOCHS\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "Cell \u001b[0;32mIn [14], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     19\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_loss \u001b[38;5;241m/\u001b[39m (index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 393\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "T = 400\n",
    "\n",
    "model_with_time = DDPM(T = T, nn_model=UNet(in_channels=1, out_channels=1, T=T+1, num_classes=10))\n",
    "_, val_losses = train(\n",
    "  model_with_time,\n",
    "  torch.optim.Adam(params=model_with_time.parameters(), lr=2e-4),\n",
    "  EPOCHS\n",
    ")\n",
    "\n",
    "plt.plot(val_losses, label=\"with time\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "torch.Size([10, 1])\n",
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m x_t \u001b[38;5;241m=\u001b[39m model_with_time\u001b[38;5;241m.\u001b[39msample(n_samples\u001b[38;5;241m=\u001b[39mn_samples, size\u001b[38;5;241m=\u001b[39mtrain_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m], w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m grid \u001b[38;5;241m=\u001b[39m make_grid(x_t, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m save_image(grid, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_without.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [41], line 48\u001b[0m, in \u001b[0;36mDDPM.sample\u001b[0;34m(self, n_samples, size, classes, w)\u001b[0m\n\u001b[1;32m     46\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x_t, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     47\u001b[0m t_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([t], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat(x_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m pred_noise_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m pred_noise_zero \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_model(x_t, t_tensor, cond, mask_zeros)\n\u001b[1;32m     50\u001b[0m pred_noise \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m w) \u001b[38;5;241m*\u001b[39m pred_noise_cond \u001b[38;5;241m-\u001b[39m w \u001b[38;5;241m*\u001b[39m pred_noise_zero\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [9], line 127\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, t, cond, mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(up_block, UpBlock):\n\u001b[0;32m--> 127\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m         ind \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [6], line 8\u001b[0m, in \u001b[0;36mSequenceWithTimeEmbedding.forward\u001b[0;34m(self, x, t, cond, mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, cond, mask):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m----> 8\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [36], line 33\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, t, condition, mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mview(batch_size, emb_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m condition \u001b[38;5;241m=\u001b[39m condition\u001b[38;5;241m.\u001b[39mview(batch_size, emb_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_residual:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_3(x) \u001b[38;5;241m+\u001b[39m conv_res\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/modules/normalization.py:273\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/nn/functional.py:2530\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2529\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/_refs/__init__.py:2956\u001b[0m, in \u001b[0;36mnative_group_norm\u001b[0;34m(input, weight, bias, batch_size, num_channels, flattened_inner_size, num_groups, eps)\u001b[0m\n\u001b[1;32m   2951\u001b[0m reduction_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m   2952\u001b[0m input_reshaped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2954\u001b[0m     [batch_size, num_groups, num_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, flattened_inner_size],\n\u001b[1;32m   2955\u001b[0m )\n\u001b[0;32m-> 2956\u001b[0m out, mean, rstd \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2957\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   2959\u001b[0m broadcast_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim))\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/_refs/__init__.py:2914\u001b[0m, in \u001b[0;36m_normalize\u001b[0;34m(a, norm_dims, eps)\u001b[0m\n\u001b[1;32m   2912\u001b[0m a_acc \u001b[38;5;241m=\u001b[39m _maybe_convert_to_dtype(a, computation_dtype)\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a_acc, TensorLike)  \u001b[38;5;66;03m# to avoid mypy error for var_mean\u001b[39;00m\n\u001b[0;32m-> 2914\u001b[0m biased_var, mean \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_mean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_acc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munbiased\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   2916\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2917\u001b[0m rstd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(biased_var \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m   2918\u001b[0m out \u001b[38;5;241m=\u001b[39m (a \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m*\u001b[39m rstd\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/_refs/__init__.py:2419\u001b[0m, in \u001b[0;36mvar_mean\u001b[0;34m(a, dim, unbiased, keepdim, correction)\u001b[0m\n\u001b[1;32m   2417\u001b[0m dim, unbiased \u001b[38;5;241m=\u001b[39m _dim_var_dispatch(dim, unbiased)\n\u001b[1;32m   2418\u001b[0m v \u001b[38;5;241m=\u001b[39m var(a, dim, unbiased, keepdim, correction\u001b[38;5;241m=\u001b[39mcorrection)\n\u001b[0;32m-> 2419\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v, m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/_refs/__init__.py:2352\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, dim, keepdim, dtype, out)\u001b[0m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;66;03m# can't use out wrapper because of this argument\u001b[39;00m\n\u001b[1;32m   2348\u001b[0m check(\n\u001b[1;32m   2349\u001b[0m     out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype,\n\u001b[1;32m   2350\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected out tensor to have dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2351\u001b[0m )\n\u001b[0;32m-> 2352\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_reduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dtype_kind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREDUCTION_OUTPUT_TYPE_KIND\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKEEP_PROMOTED_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2361\u001b[0m check(\n\u001b[1;32m   2362\u001b[0m     utils\u001b[38;5;241m.\u001b[39mis_float_dtype(dtype) \u001b[38;5;129;01mor\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mis_complex_dtype(dtype),\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     ),\n\u001b[1;32m   2368\u001b[0m )\n\u001b[1;32m   2369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, Dim):\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/_refs/__init__.py:2061\u001b[0m, in \u001b[0;36m_reduction\u001b[0;34m(a, prim, has_identity, accepts_dim_tuple, dims, keepdims, dtype, out, output_dtype_kind)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     output_shape \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mshape[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dims \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(a\u001b[38;5;241m.\u001b[39mndim)]\n\u001b[1;32m   2060\u001b[0m     broadcast_dims \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(a\u001b[38;5;241m.\u001b[39mndim) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dims]\n\u001b[0;32m-> 2061\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_in_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbroadcast_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2064\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_gpu_m1/lib/python3.9/site-packages/torch/_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 10\n",
    "x_t = model_with_time.sample(n_samples=n_samples, size=train_dataset[0][0].shape, classes=[0,1,2,3,4,5,6,7,8,9], w=2)\n",
    "grid = make_grid(x_t, nrow=10)\n",
    "save_image(grid, f\"epoch_without.png\")\n",
    "\n",
    "cols = 5\n",
    "rows = (n_samples // cols) + (0 if n_samples % cols == 0 else 1)\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "for i in range(x_t.shape[0]):\n",
    "    row = i // cols\n",
    "    axs[row, i % cols].imshow(x_t[i].permute(1,2,0).detach().cpu().numpy(), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
